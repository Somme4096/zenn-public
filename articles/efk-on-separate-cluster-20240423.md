---
title: "Elastic Cloud on Kubernetes (ECK)ã§SaaSã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ç›£è¦–ç’°å¢ƒã‚’æ§‹ç¯‰ã—ãŸè©±ã‚’ãªã‚‹ã¹ãè©³ã—ãæ›¸ã"
emoji: "ğŸ”­"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["kubernetes","fluentd","elasticsearch","kibana"]
published: false
---

# ã¯ã˜ã‚ã«
Kubernetesã§B2B SaaSãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’é‹ç”¨ã™ã‚‹ã«ã‚ãŸã‚Šã€ä¸€æ™‚é–“ã§åæ•°ä¸‡æ¡ã«ä¸Šã‚‹è†¨å¤§ãªé‡ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ã‚’åŠ¹æœçš„ã«åé›†ã—ã¦æ¤œç´¢ã‚„åˆ†æã‚’è¡Œã†æ‰‹æ®µãŒå¿…è¦ã§ã—ãŸã€‚

ãã“ã§ã€ãƒ­ã‚°åé›†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®fluentdã‚’ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã®Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ã«é…ç½®ã—ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåé›†ã—ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ã‚’ç›£è¦–ãŠã‚ˆã³å¯è¦–åŒ–ã™ã‚‹ãŸã‚ã«ã€ECKã‚’ç”¨ã„ã¦æ–°ãŸã«Elastic Stackã‚’ Linode Kubernetes Engine ä¸Šã«æ§‹ç¯‰ã—ã¾ã—ãŸã€‚

æœ¬è¨˜äº‹ã§ã¯ã€fluentdã«ã‚ˆã‚‹ãƒ­ã‚°ã®åé›†ãŠã‚ˆã³ElasticSearchã¸ã®é€ä¿¡ã€Elastic Stackã®æ§‹ç¯‰ãŠã‚ˆã³Kibanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®Ingressã«ã‚ˆã‚‹å…¬é–‹ã¨ç°¡ç•¥ãªSSLèªè¨¼ã¾ã§ã‚«ãƒãƒ¼ã—ã¾ã™ã€‚

## ECKã¨ã¯ï¼Ÿ
**ECK**(**E**lastic **C**loud on **K**ubernetes)ã¯ElasticSearchã‚„Kibanaãªã©ã‚’å«ã‚€Elastic Stackã®CRD/Operatorã§ã€Kubernetesã¸ã®Elastic Stackã®ãƒ‡ãƒ—ãƒ­ã‚¤ã¨ç®¡ç†ã‚’åŠ¹ç‡åŒ–ã—ã¾ã™ã€‚

å…·ä½“çš„ã«ã¯ã€ElasticSearchã‚„Kibanaã€APM serverãªã©å€‹ã€…ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚«ã‚¹ã‚¿ãƒ ãƒªã‚½ãƒ¼ã‚¹ã‚’å®šç¾©ã—ã€å®šç¾©ã•ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ç®¡ç†ã‚’è‡ªå‹•åŒ–ã—ã¾ã™ã€‚
:::message
(24/07/23)ç¾åœ¨ã€å…¬å¼ã«æ¨å¥¨ã•ã‚Œã‚‹Elastic Stackã®Kubernetesã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤æ–¹æ³•ã§ã™ã€‚
:::

## å‹•ä½œç’°å¢ƒ
#### åé›†å´ã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆSaaSãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
- LKE (Linode Kubernetes Engine) Dedicated 32GB
:::message
å®Ÿéš›ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹åé›†ç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®fluentdã¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡~200mbã»ã©ã®å°ã•ã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚ã‚‹ãŸã‚ã€å¤§æŠµã®ç’°å¢ƒã§å‹•ä½œã§ãã¾ã™ã€‚ãã®ãŸã‚ã€ã“ã“ã§ã¯åé›†å´ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚¹ãƒšãƒƒã‚¯ã¯æ˜è¨˜ã—ã¾ã›ã‚“ã€‚
:::
#### Elastic Stackç”¨ã‚¯ãƒ©ã‚¹ã‚¿
- LKE (Linode Kubernetes Engine) Dedicated 8GB plan x 1
  - 2 CPU
  - 8GB RAM
  - 160GB Storage
  - 5TB Transfer
  - 40/5Gbps Network In/Out
- Linode Block Storageï¼ˆå¾“é‡èª²é‡‘åˆ¶ï¼‰
  - å—ä¿¡ã—ãŸãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã«ä½¿ç”¨

:::message alert
LKEã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹å ´åˆã€Dedicated 8GBä»¥ä¸‹ã®ãƒ—ãƒ©ãƒ³ã¯ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ã§Elastic StackãŒèµ·å‹•ã§ãã¾ã›ã‚“ã€‚å¯èƒ½ã§ã‚ã‚Œã°ã€Dedicated 8GB x3å°ã®æ§‹æˆãŒæœ›ã¾ã—ã„ã¨æ€ã‚ã‚Œã¾ã™ã€‚
:::
# 1.Elastic Stackã®æ§‹ç¯‰
:::message
(**24/07/23**)ç¾åœ¨ã€æœ€æ–°ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã‚ã‚‹8.14.3ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã™ã€‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹Kubernetesãƒãƒ¼ã‚¸ãƒ§ãƒ³ãªã©ã¯ã“ã¡ã‚‰(https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
:::

## 1.1 å‰æã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
å…¬å¼ã‚¬ã‚¤ãƒ‰ã‚’å‚è€ƒã«ã—ã¦ã‚«ã‚¹ã‚¿ãƒ ãƒªã‚½ãƒ¼ã‚¹(CRD)ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

```bash
kubectl create -f https://download.elastic.co/downloads/eck/2.13.0/crds.yaml
```
```
customresourcedefinition.apiextensions.k8s.io/agents.agent.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/apmservers.apm.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/beats.beat.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/elasticmapsservers.maps.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/elasticsearches.elasticsearch.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/enterprisesearches.enterprisesearch.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/kibanas.kibana.k8s.elastic.co created
customresourcedefinition.apiextensions.k8s.io/logstashes.logstash.k8s.elastic.co created
```
ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã™ã€‚
```bash
kubectl apply -f https://download.elastic.co/downloads/eck/2.13.0/operator.yaml
```
## 1.2 ElasticSearchã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

æ¬¡ã«ã€ElasticSearchã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ã“ã“ã§ã¯`default`ãƒãƒ¼ãƒ‰ã‚’ä¸‰ã¤ä½œæˆã—ã€å„ãƒãƒ¼ãƒ‰ã«300Giã®Linode Block Storageã‚’ä¸ãˆã¦ã„ã¾ã™ã€‚åå‰ã¯`logging-dashboard`ã¨ã—ã¾ã™ã€‚
:::message
ä¸Šè¨˜ã®`default`ãƒãƒ¼ãƒ‰ã¯ElasticSearchã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å†…ãƒãƒ¼ãƒ‰ã§ã€Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒãƒ¼ãƒ‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚Kuberneteså†…ã§ã¯podã¨ã—ã¦æ‰±ã‚ã‚Œã¦ã„ã¾ã™ã€‚
:::
Linode Block Storageä»¥å¤–ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯é©å®œ`storageClassName`ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚ãƒ†ã‚¹ãƒˆç”¨é€”ã«ãƒ­ãƒ¼ã‚«ãƒ«ãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚’ä½¿ç”¨ã—ãŸã„å ´åˆã¯`local-path-provisioner`ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚
@[card](https://github.com/rancher/local-path-provisioner)

```yaml:elasticsearch.yaml
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: logging-dashboard
spec:
  version: 8.14.3
  nodeSets:
  - name: default
    count: 3
    config:
      node.roles: ["master", "data"]
      node.store.allow_mmap: false
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data # Do not change this name unless you set up a volume mount for the data path.
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 300Gi
        storageClassName: linode-block-storage-retain
```
ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ãŸã‚‰applyã—ã¾ã™ã€‚
```bash
kubectl apply -f elasticsearch.yaml
```
ãƒ˜ãƒ«ã‚¹çŠ¶æ…‹ã‚’ç¢ºã‹ã‚ã¾ã™ã€‚
```
$ kubectl get elasticsearch

NAME                HEALTH   NODES   VERSION   PHASE   AGE
logging-dashboard   green    3       8.14.3    Ready   90s
```


å•é¡Œãªãå‹•ä½œã—ã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚
## 1.3 Kibanaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§Kibanaã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚æ³¨æ„ç‚¹ã¨ã—ã¦ã€`metadata.name`ã¨`elasticsearchRef`ã®å€¤ã¯elasticsearchã®æ§‹æˆãƒ•ã‚¡ã‚¤ãƒ«ã®`metadata.name`ã®å€¤ã¨åŒã˜ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚`spec.count`ã§åŒæ™‚ã«å‡ºç¾ã™ã‚‹Kibanaã®podã®æ•°ã‚’æŒ‡å®šã§ãã¾ã™ã€‚
```yaml:kibana.yaml
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: logging-dashboard
spec:
  version: 8.14.3
  count: 2
  elasticsearchRef:
    name: logging-dashboard
```
ãƒ•ã‚¡ã‚¤ãƒ«ã‚’applyã—ã¾ã™ã€‚
```bash
kubectl apply -f kibana.yaml
```
ãƒ˜ãƒ«ã‚¹çŠ¶æ…‹ã‚’ç¢ºã‹ã‚ã¾ã™ã€‚
```
$ kubectl get kibana

NAME                HEALTH   NODES   VERSION   AGE
logging-dashboard   green    1       8.14.3    90s
```

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¯ç‰¹ã«å•é¡Œãªãå®Œäº†ã—ãŸã‚ˆã†ã§ã™ã€‚è©¦ã—ã«ã€Kibanaã®Serviceã‚’å–å¾—ã—ã¦`port-forwarding`ã§å‹•ä½œç¢ºèªã—ã¦ã¿ã¾ã™ã€‚
```
$ kubectl get svc

NAME                                 TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)          AGE
kubernetes                           ClusterIP      10.128.0.1       <none>          443/TCP          116d
logging-dashboard-es-default         ClusterIP      None             <none>          9200/TCP         2d15h
logging-dashboard-es-http            ClusterIP      10.128.5.221     <none>          9200/TCP         2d15h
logging-dashboard-es-internal-http   ClusterIP      10.128.238.88    <none>          9200/TCP         2d15h
logging-dashboard-es-transport       ClusterIP      None             <none>          9300/TCP         2d15h
logging-dashboard-kb-http            ClusterIP      10.128.185.177   <none>          5601/TCP         2d15h
```


Kibanaã®Serviceã§ã‚ã‚‹`logging-dashboard-kb-http`ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚
æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§Kibanaã«æ¥ç¶šã—ã¦ã¿ã¾ã™ã€‚
:::message
æ³¨:kibanaã®Serviceã®åç§°ã¯`kibanaã®æ§‹æˆãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹metadata.nameã®å€¤`-kb-httpã®ã‚ˆã†ãªå½¢å¼ã«ãªã£ã¦ã„ã¾ã™ã€‚
:::
```
kubectl port-forward service/logging-dashboard-kb-http 5601
```
ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã„ã¦ã¿ã¾ã™ã€‚
@[card](https://localhost:5601)
å•é¡Œãªãè¡¨ç¤ºã§ãã¾ã—ãŸã€‚
![](/images/efk-on-separate-cluster/image.png)
## 1.3 Ingressã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ç”¨ã—ã¦Kibanaã‚’å…¬é–‹ã™ã‚‹

å…ˆã»ã©`kubectl port-forwarding`ã§Kibanaã®å‹•ä½œç¢ºèªãŒã§ãã¾ã—ãŸãŒã€ç¾åœ¨ã®çŠ¶æ…‹ã§ã¯ã‚¯ãƒ©ã‚¹ã‚¿å¤–éƒ¨ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã€‚ã¨ã„ã†ã®ã‚‚ã€å†åº¦`kubectl get svc`ã§ç¢ºèªã™ã‚‹ã¨
```
NAME                                 TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)          AGE
logging-dashboard-es-default         ClusterIP      None             <none>          9200/TCP         41h
logging-dashboard-es-http            ClusterIP      10.128.5.221     <none>          9200/TCP         41h
logging-dashboard-es-internal-http   ClusterIP      10.128.238.88    <none>          9200/TCP         41h
logging-dashboard-es-transport       ClusterIP      None             <none>          9300/TCP         41h
logging-dashboard-kb-http            ClusterIP      10.128.185.177   <none>          5601/TCP         41h
```
å¤–éƒ¨ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã—ãŸã„Kibanaã®Serviceã§ã‚ã‚‹`logging-dashboard-kb-http`ã¯ã‚¿ã‚¤ãƒ—ãŒã‚¯ãƒ©ã‚¹ã‚¿ã®å†…éƒ¨é€šä¿¡ã«ä½¿ã‚ã‚Œã‚‹`ClusterIP`ã§ã€`EXTERNAL-IP`ãŒç©ºæ¬„ã«ãªã£ã¦ã„ã¦ã„ã‚‹ã“ã¨ã‹ã‚‰å¤–éƒ¨ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹å…¬é–‹IPã‚¢ãƒ‰ãƒ¬ã‚¹ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ãªã„ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

ã¨åŒæ§˜ã«ã€ãƒ­ã‚°åé›†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã‚ã‚‹fluentdã¯Kibanaã§ã¯ãªãElasticSearchã«ãƒ­ã‚°ã‚’é€ä¿¡ã™ã‚‹ã®ã§ã€ElasticSearchã®ã‚µãƒ¼ãƒ“ã‚¹ã‚‚åŒæ§˜ã«å…¬é–‹ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚æ–°ã—ãåˆ¥ã€…ã«`LoadBalancer`ã‚’å‰²ã‚Šå½“ã¦ã‚‹æ‰‹æ³•ã‚‚ã‚ã‚Šã¾ã™ãŒã€ã“ã®è¨˜äº‹ã§ã¯`Ingress`ãŠã‚ˆã³`Ingress-Controller`ã‚’ç”¨ã„ã¦é©åˆ‡ã«å¤–éƒ¨ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’è©²å½“ã®å†…éƒ¨Serviceã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã‚’å–ã‚Šã¾ã™ã€‚

åŒæ™‚ã«ã€cert-managerã‚’åˆ©ç”¨ã—ã¦TLSè¨¼æ˜æ›¸ã®ç™ºè¡ŒãŠã‚ˆã³ç®¡ç†ã‚’è‡ªå‹•çš„ã«è¡Œã†ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã‚’è¡Œã„ã¾ã™ã€‚ä¸€èˆ¬çš„ãªWebã‚µãƒ¼ãƒ“ã‚¹ã¯HTTPSãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚å½“ç„¶ã€Kibanaã‚‚ãã®ä¾‹å¤–ã§ã¯ãªãã€å¿…ç„¶çš„ã«TLSç½²åãŒå¿…è¦ã¨ãªã‚Šã¾ã™ã€‚

### 1.3.1 Ingressã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹
`Ingress`ã‚¿ã‚¤ãƒ—ã®Serviceã‚’æ‰±ã†ã«ã¯Ingressã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ãŒ**å¿…ãš**å¿…è¦ã§ã™ã€‚Ingressã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã¯å¤šæ•°å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ãŒã€ã“ã®è¨˜äº‹ã§ã¯Ingress Nginx Controllerã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
@[card](https://kubernetes.github.io/ingress-nginx/deploy/)
:::message
ã“ã“ã§ã¯helmã‚’ä½¿ç”¨ã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã„ã¾ã™ãŒã€ä¸€èˆ¬çš„ãªãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’åˆ©ç”¨ã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚è©³ã—ãã¯ingress-nginxã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
:::
ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚
```
helm upgrade --install ingress-nginx ingress-nginx \
  --repo https://kubernetes.github.io/ingress-nginx \
  --namespace ingress-nginx --create-namespace
```
ã“ã‚Œã§ã€`ingress-nginx`namespaceã«Ingress Nginx ControllerãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¾ã—ãŸã€‚
`kubectl get svc -n ingress-nginx`ã§ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```
NAME                                 TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)                      AGE
ingress-nginx-controller             LoadBalancer   10.128.229.175   xx.xx.xxx.xx   80:30350/TCP,443:31867/TCP   43h
ingress-nginx-controller-admission   ClusterIP      10.128.142.155   <none>         443/TCP                      43h
```
å‹•ä½œã‚’ç¢ºèªã§ãã¾ã—ãŸã€‚

### 1.3.2 cert-managerã¨Let's Encryptã§Kibanaã®TLSèªè¨¼ã‚’è¡Œã†

Let's EncryptãŒç„¡æ–™ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‹ã¤è‡ªå‹•åŒ–ã•ã‚ŒãŸè¨¼æ˜æ›¸ã®ç™ºè¡Œã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã—ã¦ã„ã‚‹ã®ã§ã€cert-managerã§è‡ªå‹•çš„ã«Let's Encrypt(HTTP01 Challenge)ã«ã‚ˆã‚‹è¨¼æ˜æ›¸ã®ç™ºè¡Œã¨æœŸé™åˆ‡ã‚Œã®è¨¼æ˜æ›¸ã®æ›´æ–°ã‚’è¡Œãˆã‚‹ã‚ˆã†ã«è¨­å®šã—ã¾ã™ã€‚

ã¾ãšã¯cert-managerã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚ã™ã§ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹æ–¹ã¯æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«ãŠé€²ã¿ãã ã•ã„ã€‚
@[card](https://cert-manager.io/docs/installation/)
:::message
ã“ã“ã§ã¯helmã‚’ä½¿ç”¨ã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã„ã¾ã™ãŒã€ä¸€èˆ¬çš„ãªãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’åˆ©ç”¨ã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚è©³ã—ãã¯cert-managerã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
:::
```bash
$ helm repo add jetstack https://charts.jetstack.io --force-update
>> "jetstack" has been added to your repositories

$ helm repo update
>> Successfully got an update from the "jetstack" chart repository

$ helm install \
  cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.15.1 \
  --set crds.enabled=true
>> cert-manager v1.15.1 has been deployed successfully!
```
æ¬¡ã«ã€`ClusterIssuer`ã‚’ä½œæˆã—ã¾ã™ã€‚cert-managerã®Issuerã«ã¯äºŒç¨®é¡ã‚ã‚Šã€ã“ã“ã§ã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¬ãƒ™ãƒ«ã®ã‚¹ã‚³ãƒ¼ãƒ—ã§ä½¿ç”¨ã§ãã‚‹ClusterIssuerã‚’ä½¿ã„ã¾ã™ã€‚

```yaml:clusterissuer.yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    email: youremail@youremail.com
    server: https://acme-v02.api.letsencrypt.org/directory
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: nginx
          serviceType: ClusterIP
```
æ³¨æ„ã™ã¹ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ç°¡å˜ã«èª¬æ˜ã—ã¾ã™ã€‚
- `acme.email`:è‡ªåˆ†ã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’è¨˜å…¥ã—ã¦ãã ã•ã„ã€‚
- `acme.server`:ä½¿ç”¨ã™ã‚‹acmeã®ã‚µãƒ¼ãƒãƒ¼ã€‚ã“ã“ã§ã¯Let's Encryptã®Prod(Production)ã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ãŒã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆå›æ•°ã«åˆ¶é™ãŒã‚ã‚‹ã®ã§ã€ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯Let's Encryptã®stagingç’°å¢ƒ(https://acme-staging-v02.api.letsencrypt.org/directory)ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

:::message alert
Let's Encryptã®stagingç’°å¢ƒã«ã¦ç™ºè¡Œã•ã‚Œã‚‹è¨¼æ˜æ›¸ã¯ç™ºè¡Œå´ãŒå®Œå…¨ãªæƒ…å ±ã‚’è¨˜è¿°ã—ã¦ã„ãªã„ã®ã§æ­£å¼ã«ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚ãã¾ã§ãƒ†ã‚¹ãƒˆç”¨ã¨ã—ã¦ã”åˆ©ç”¨ãã ã•ã„ã€‚
:::
- `acme.solvers[0].http01.ingress.class`:ä½¿ç”¨ã—ã¦ã„ã‚‹Ingressã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã®å€¤ã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚ã“ã®è¨˜äº‹ã§ã¯`nginx`ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã§cert-managerã¯port 80ã«ingressã‚’é…ç½®ã—ã€acmeã‚µãƒ¼ãƒãƒ¼ã«HTTP-01èªè¨¼ã‚’ã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®èª¿æ•´ãŒçµ‚ã‚ã‚Šã¾ã—ãŸã‚‰applyã—ã¾ã—ã‚‡ã†ã€‚
```bash
kubectl apply -f clusterissuer.yaml
```
ã¤ã„ã§ã«ç¢ºèªã‚‚ã—ã¦ã¿ã¾ã™ã€‚
```bash
$ kubectl get ClusterIssuer

NAME               READY   AGE
letsencrypt-prod   True    13m
```


# Kibanaã‚’Ingressã§å…¬é–‹ã™ã‚‹
ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§`ingress`Serviceã‚’ä½œæˆã—ã¾ã™ã€‚
```yaml:ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/backend-protocol: HTTPS
  name: kibana
spec:
  ingressClassName: nginx
  rules:
  - host: your domain
    http:
      paths:
      - backend:
          service:
            name: logging-dashboard-kb-http
            port:
              number: 5601
        path: /
        pathType: ImplementationSpecific
      - path: /elasticsearch
        backend:
          serviceName: logging-dashboard-es-http 
          servicePort: 9200
  tls:
  - hosts:
    - your domain
    secretName: your domain
```

æ³¨æ„ã™ã¹ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ä»¥ä¸‹ã«èª¬æ˜ã—ã¾ã™ã€‚
- `your domain`:ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ã‚‚ã—æ‰‹æŒã¡ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã‚‰ã€ãƒ†ã‚¹ãƒˆç”¨é€”ã«`nip.io`ã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚å…ˆã»ã©å–å¾—ã—ãŸ`ingress-nginx-controller`Serviceã®`External-IP`ã®å€¤ã‚’`your domain`ã®ã‚ˆã†ã«å½“ã¦ã¯ã‚ã‚‹ã“ã¨ã§DNSãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãŒã§ãã¾ã™ã€‚è©³ã—ãã¯ https://nip.io/ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
- `your Ingressã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼`:ä½¿ç”¨ã—ã¦ã„ã‚‹Ingressã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã®class nameã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚ã“ã“ã§ã¯`nginx`ã§ã™ãƒ»

# 2.åé›†å´ã®ä½œæ¥­
## 2.1 namespaceã‚’ä½œæˆã™ã‚‹
ä»–ã®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãŒ`default`ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€namespaceã‚’`default`ã¨åˆ†ã‘ã¦`fluent`ã§ä½œæˆã—ã¾ã™ã€‚
``` yaml:fluent-namespace.yaml
kind: Namespace
apiVersion: v1
metadata:
  name: fluent
```
applyã—ã¾ã™ã€‚
```bash
kubectl apply -f fluent-namespace.yaml
```
## 2.2 fluentdã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›´ã™ã‚‹ãŸã‚ã®`ConfigMap`ã‚’ä½œæˆã™ã‚‹
fluentdã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§`var/log/containers/`ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦jsonç”¨ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã¯dockerã®ãƒ­ã‚°ãŒjsonå½¢å¼ã§ã‚ã‚‹ãŸã‚ã§ã™ã€‚
ã§ã™ãŒã€ã‚‚ã—containerdã‚„cri-oã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã€ãƒ­ã‚°ã®å½¢å¼ã¯dockerã®ãã‚Œã¨å…¨ãç•°ãªã‚Šã¾ã™ã€‚ã‚ˆã£ã¦ã€å°‚ç”¨ã®criãƒ‘ãƒ¼ã‚µãƒ¼(https://github.com/fluent/fluent-plugin-parser-cri)ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ã©ã®ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã¹ããªã®ã‹ä¸ç¢ºã‹ã§ã—ãŸã‚‰ã€`kubectl exec -it yourpodname -- sh`ã§podã«æ¥ç¶šã—ã€`/var/log/`ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦—ã„ã¦ã¿ã¦ãã ã•ã„ã€‚ã‚‚ã—ãƒ­ã‚°ã®å½¢å¼ãŒ

```
2020-10-10T00:10:00.333333333Z stdout F Hello Fluentd
```
ã®ã‚ˆã†ã«ãªã£ã¦ã„ãŸã‚‰ã€ä»¥ä¸‹ã‚’å‚è€ƒã«fluentdã®è¨­å®šã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚ã‚‚ã—jsonå½¢å¼ã§ã—ãŸã‚‰ã“ã®æ‰‹é †ã¯å¿…è¦ã‚ã‚Šã¾ã›ã‚“ã€‚
`ConfigMap`ç”¨ã®ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚
```yaml:fluent-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: tail-container-parse-conf
  namespace: fluent
data:
  tail_container_parse.conf: |
```
applyã—ã¾ã™ã€‚
```
kubectl apply -f fluent-configmap.yaml
```

## 2.3  Elastic Stackã‚¯ãƒ©ã‚¹ã‚¿ã®æƒ…å ±ã‚’è¨˜è¼‰ã™ã‚‹Secretã‚’ä½œæˆã™ã‚‹

fluentdãŒElasticSearchã«ãƒ­ã‚°ã‚’é€ä¿¡ã™ã‚‹éš›ã«ã¯ElasticSearchã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¨ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’ç›´æ¥fluentdã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜è¿°ã™ã‚‹ã®ã¯å®‰å…¨æ€§ã¨æŸ”è»Ÿæ€§ã«æ¬ ã‘ã¾ã™ã®ã§ã€ã“ã“ã§ã¯Secretã‚’ä½¿ç”¨ã—ã¦å…ˆã»ã©å–å¾—ã—ãŸãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¨ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’è¨˜è¿°ã—ã¾ã™ã€‚

ã¾ãšã€Opaqueã‚¿ã‚¤ãƒ—ã®Secretã«æ ¼ç´ã•ã‚Œã‚‹æƒ…å ±ã¯`base64`ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã®ã§ã€å…ˆã«ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¨ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§`base64`ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¾ã—ã‚‡ã†ã€‚
```bash
echo -n 'fluentd' | base64
```
```bas2
echo -n 'mypassword' | base64
```

:::message
æ³¨ï¼š`''`ã¯å–ã‚Šå¤–ã•ãªã„ã§ãã ã•ã„ã€‚
:::
æ¬¡ã«ã€å…ˆã»ã©ã®ã‚³ãƒãƒ³ãƒ‰ã§å‡ºåŠ›ã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼åã¨ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰çµæœã‚’ã€ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®è©²å½“é …ç›®ã«å…¥åŠ›ã—ã¾ã™ã€‚
```yaml:fluent-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: elastic-secret
  namespace: fluent
type: Opaque
data:
  username: 
  password: 
```

applyã—ã¾ã™ã€‚
```bash
kubectl apply -f fluent-secret.yaml
```

## 2.4 fluentdã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹
ã„ã‚ˆã„ã‚ˆå¤§è©°ã‚ã§ã™ã€‚ä»¥ä¸‹ã«å¤‰æ›´ã™ã¹ãã®`metadata.name`ã®å€¤ã¨åŒã˜ã«è¨­å®š`spec.count`ã¯kibanaã®åŒæ™‚ã«å‡ºç¾ã™ã‚‹podã®æ•°ã‚’æŒ‡å®šã§ãã¾ã™ã€‚ã—ã¦ãã ã•ã„ã€‚ã®ä¸€è¦§ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚
- `containers.env.name: FLUENT_ELASTICSEARCH_HOST`
  - å…ˆã»ã©å–å¾—ã—ãŸElasticsearchã®EXTERNAL-IPã‚’è¨˜å…¥ã—ã¦ãã ã•ã„ã€‚
- 

```yaml:fluent.yaml
---
apiVersion: v1
kind: ServiceAccoun2
metadata:
  name: fluentd
  namespace: fluent
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - namespaces
  verbs:
  - get
  - list
  - watch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fluentd
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: fluentd
  namespace: fluent
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: fluent
  labels:
    k8s-app: fluentd-logging
    version: v1
spec:
  selector:
    matchLabels:
      k8s-app: fluentd-logging
      version: v1
  template:
    metadata:
      labels:
        k8s-app: fluentd-logging
        version: v1
    spec:
      serviceAccount: fluentd
      serviceAccountName: fluentd
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1.17-debian-elasticsearch8-1
        env:
          - name: K8S_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name:  FLUENT_ELASTICSEARCH_HOST
            value: "yourelasticsearchhost"
          - name:  FLUENT_ELASTICSEARCH_PORT
            value: "9200"
          - name: FLUENT_ELASTICSEARCH_SCHEME
            value: "https"
          - name: FLUENTD_SYSTEMD_CONF
            value: disable
          # Option to configure elasticsearch plugin with self signed certs
          # ================================================================
          - name: FLUENT_ELASTICSEARCH_SSL_VERIFY
            value: "true"
          # Option to configure elasticsearch plugin with tls
          # ================================================================
          - name: FLUENT_ELASTICSEARCH_SSL_VERSION
            value: "TLSv1_2"
          # X-Pack Authentication
          # =====================
          - name: FLUENT_ELASTICSEARCH_USER
            valueFrom:
              secretKeyRef:
                name: elastic-secret
                key: username
          - name: FLUENT_ELASTICSEARCH_PASSWORD
            valueFrom:
              secretKeyRef:
                name: elastic-secret
                key: password
          - name: FLUENT_CONTAINER_TAIL_EXCLUDE_PATH
            value: /var/log/containers/fluent*
          - name: FLUENT_CONTAINER_TAIL_PATH
            value: /var/log/containers/*.log
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: tail-container-parse-conf-volume
          mountPath: /fluentd/etc/tail_container_parse.conf
          subPath: tail_container_parse.conf
        # When actual pod logs in /var/lib/docker/containers, the following lines should be used.
        - name: dockercontainerlogdirectory
          mountPath: /var/lib/docker/containers
          readOnly: true
        # When actual pod logs in /var/log/pods, the following lines should be used.
        # - name: dockercontainerlogdirectory
        #   mountPath: /var/log/pods
        #   readOnly: true
      terminationGracePeriodSeconds: 45
      volumes:
      - name: tail-container-parse-conf-volume
        configMap:
          name: tail-container-parse-conf
      - name: varlog
        hostPath:
          path: /var/log
      # When actual pod logs in /var/lib/docker/containers, the following lines should be used.
      - name: dockercontainerlogdirectory
        hostPath:
          path: /var/lib/docker/containers
      # When actual pod logs in /var/log/pods, the following lines should be used.
      # - name: dockercontainerlogdirectory
      #   hostPath:
      #     path: /var/log/pods
```

